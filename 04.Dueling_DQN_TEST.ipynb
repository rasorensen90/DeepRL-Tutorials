{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dueling Deep Q Network TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import gym, math, glob\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from environments.BHS.environment_v4_2 import Environment\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import NNConv\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from utils.wrappers import *\n",
    "from agents.DQN import Model as DQN_Agent\n",
    "from utils.ReplayMemory import ExperienceReplayMemory\n",
    "\n",
    "from utils.hyperparameters import Config\n",
    "from utils.plot import plot_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start = 1.0\n",
    "config.epsilon_final = 0.01\n",
    "config.epsilon_decay = 300000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA=0.99\n",
    "config.LR=1e-4\n",
    "config.USE_PRIORITY_REPLAY = True\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1000\n",
    "config.EXP_REPLAY_SIZE = 50000\n",
    "config.BATCH_SIZE = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 100\n",
    "config.MAX_FRAMES=1000000\n",
    "config.UPDATE_FREQ = 1\n",
    "\n",
    "#Nstep controls\n",
    "config.N_STEPS=5\n",
    "\n",
    "#data logging parameters\n",
    "config.ACTION_SELECTION_COUNT_FREQUENCY = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BHSDuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_outputs, graph, edgelist, edge_attr, edge_nodes):\n",
    "        super(BHSDuelingDQN, self).__init__()\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.input_shape[0] = graph.number_of_nodes()\n",
    "        self.num_actions = num_outputs # a vector of the number of actions at each diverter\n",
    "        self.edge = edgelist\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_nodes = edge_nodes\n",
    "        \n",
    "        nn1 = nn.Sequential(nn.Linear(1, 64), nn.ReLU(), nn.Linear(64, self.input_shape[1]*128)) # edge attribute neural network\n",
    "        self.conv1 = NNConv(self.input_shape[1], 128, nn1)\n",
    "        \n",
    "        self.adv = nn.Linear(self.feature_size(), sum(self.num_actions)) # Might be an idea to add another fc layer here\n",
    "\n",
    "        self.val1 = nn.Linear(self.feature_size(), 64)\n",
    "        self.val2 = nn.Linear(64, 64)\n",
    "        self.val3 = nn.Linear(64, len(self.num_actions))\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # x comes in as an N x H x C shape (N is batch size, H is number of elements (height), C is number of features (channels))\n",
    "        x_shape = x.shape\n",
    "        \n",
    "        #start_down=timer()\n",
    "        x_down = torch.zeros([x_shape[0],self.input_shape[0],x_shape[2]])\n",
    "        edge_occu = [0]*len(self.edge_nodes)\n",
    "        for n in range(x_shape[0]):\n",
    "            i = 0\n",
    "            for k in range(x_shape[1]):\n",
    "                if (k in self.edge):\n",
    "                    x_down[n][i] = x[n][k]\n",
    "                    i += 1\n",
    "                    break\n",
    "          #      else:\n",
    "          #          for e in range(len(self.edge_nodes)):\n",
    "          #              if k in self.edge_nodes[e]:\n",
    "          #                  if len(torch.nonzero(x[n][k],as_tuple=False)) != 0:\n",
    "          #                      edge_occu[e] += 1\n",
    "          #                  break\n",
    "\n",
    "        edge_attr = torch.Tensor(list(np.array(self.edge_attr) - np.array(edge_occu)))\n",
    "        x_shape = x_down.shape\n",
    "        x = x_down.view(x_shape[0]*x_shape[1],x_shape[2]) # set shape of x to [N*H, C] to get the shape of a Graph batch\n",
    "        #time_down = timer()-start_down\n",
    "        #print('TIME DOWN = ',time_down, x_shape[0])\n",
    "        \n",
    "        x = F.relu(self.conv1(x, self.edge, edge_attr))\n",
    "        \n",
    "        x = x.view(x_shape[0], -1) # set shape of x to [N,:] to keep the batch size\n",
    "        \n",
    "        adv = F.relu(self.adv(x))\n",
    "        adv = adv.view(adv.size(0),len(self.num_actions),-1)\n",
    "            \n",
    "        val = F.relu(self.val1(x))\n",
    "        val = F.relu(self.val2(val))\n",
    "        val = self.val3(val)\n",
    "        \n",
    "        return val.unsqueeze(-1).expand_as(adv) + adv - adv.mean(-1).unsqueeze(-1).expand_as(adv)\n",
    "    \n",
    "    def feature_size(self):\n",
    "        x = self.conv1(\n",
    "                    torch.zeros([self.input_shape[0],self.input_shape[1]],dtype=torch.float),\n",
    "                    torch.zeros([self.edge.shape[0],self.edge.shape[1]], dtype=torch.long),\n",
    "                    torch.zeros([self.edge.shape[1]],dtype=torch.float))\n",
    "        x = x.view(1, -1)\n",
    "        x = x.size(1)\n",
    "        return x\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        #ignore this for now\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(DQN_Agent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None, log_dir='tmp/TEST/'):\n",
    "        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n",
    "\n",
    "    def declare_networks(self):\n",
    "        graph = self.env.down_graph.to(self.device)\n",
    "        edgelist = self.env.edgelist_down.to(self.device)\n",
    "        edge_attr = self.env.edge_attr\n",
    "        edge_nodes = self.env.edge_nodes\n",
    "        self.model = BHSDuelingDQN(self.env.observation_space.shape, self.env.action_space.nvec, graph, edgelist, edge_attr, edge_nodes)\n",
    "        self.target_model = BHSDuelingDQN(self.env.observation_space.shape, self.env.action_space.nvec, graph, edgelist, edge_attr, edge_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0.003295197699999978 0.0014522843999999412 0.06534181839999997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d82d17ac0b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mstart_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mtime_update\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Github\\DeepRL-Tutorials\\agents\\DQN.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, s, a, r, s_, frame)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mbatch_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# Optimize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Github\\DeepRL-Tutorials\\agents\\DQN.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, batch_vars, s_)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriority_replay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Github\\DeepRL-Tutorials\\agents\\BaseAgent.py\u001b[0m in \u001b[0;36mMSE\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=timer()\n",
    "\n",
    "log_dir = \"tmp/TEST/\"\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*td.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*sig_param_mag.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*action_log.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "class Arg_parser():\n",
    "    def __init__(self):\n",
    "        self.max_timesteps = 10000000\n",
    "        self.envtype = 'env_2_0'\n",
    "        self.tb_log_name = 'DQN'\n",
    "        self.steplimit = 200\n",
    "        self.log_interval = 1000\n",
    "        self.step_penalty = None\n",
    "        self.trasum_scale = None\n",
    "        self.destination_score = None\n",
    "        self.numtotes = 30\n",
    "        self.randomize_numtotes = False\n",
    "        self.RL_diverters = None\n",
    "    \n",
    "args = Arg_parser()\n",
    "\n",
    "        \n",
    "env_id = args.envtype\n",
    "env    = Environment(args) #make_atari(env_id)\n",
    "env    = bench.Monitor(env, os.path.join(log_dir, env_id))\n",
    "# env    = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n",
    "# env    = ImageToPyTorch(env)\n",
    "model  = Model(env=env, config=config, log_dir=log_dir)\n",
    "\n",
    "episode_reward = 0\n",
    "time_get, time_step, time_update = 0,0,0\n",
    "observation = env.reset(total = True)\n",
    "for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "    epsilon = config.epsilon_by_frame(frame_idx)\n",
    "    \n",
    "    start_get=timer()\n",
    "    action = model.get_action(observation, epsilon)[0]\n",
    "    time_get += timer()-start_get\n",
    "    \n",
    "    start_step=timer()\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _, actual_action = env.step(action)\n",
    "    observation = None if env.deadlock else observation     \n",
    "    time_step += timer()-start_step\n",
    "    \n",
    "    action = actual_action\n",
    "    model.save_action(action, frame_idx) #log action selection\n",
    "    \n",
    "    start_update=timer()\n",
    "    model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    episode_reward += reward\n",
    "    time_update += timer()-start_update   \n",
    "    \n",
    "    if frame_idx % args.steplimit*10 == 0:\n",
    "        total_reset = True\n",
    "    else:\n",
    "        total_reset = env.deadlock\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        model.reset_hx()\n",
    "        \n",
    "        observation = env.reset(total=total_reset)\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "    \n",
    "    \n",
    "    if frame_idx % args.log_interval == 0:\n",
    "        model.save_w()\n",
    "        try:\n",
    "            clear_output(True)\n",
    "            print(frame_idx)\n",
    "            print(time_get/args.log_interval, time_step/args.log_interval, time_update/args.log_interval)\n",
    "            time_get, time_step, time_update = 0,0,0\n",
    "            plot_all_data(log_dir, env_id, 'BHSDuelingDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), save_filename='Results/TEST/Results.svg', ipynb=False)\n",
    "        except IOError:\n",
    "            pass\n",
    "\n",
    "model.save_w()\n",
    "env.close()\n",
    "plot_all_data(log_dir, env_id, 'BHSDuelingDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), save_filename='Results/TEST/Results.svg', ipynb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
